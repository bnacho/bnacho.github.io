# 선형회귀(Linear Regression)
- 두 개 또는 그 이상의 변수 간 인과관계의 패턴을 원래 모습과 가장 가깝게 추정하는 분석 방법
- 원인과 결과로 이루어져 있으며 원인은 독립변수 x, 결과는 종속변수 y로 정의한다.
![스크린샷 2024-11-29 232548](https://github.com/user-attachments/assets/bfb454db-767b-405b-b77c-012f8a2da270)

## 선형회귀에 사용되는 개념들
- 학습을 통한 예측함수로 새로운 특징벡터 입력 x에 대한 목표값 출력 y를 예측한다.
- 즉, 위 사진처럼 직선으로 표현된다.
- 아래의 면적에 따른 집값을 그래프의 점으로 나타내고 이를 대표하는 직선을 그린다면 여러 직선을 그릴 수 있다.
![스크린샷 2024-11-29 232821](https://github.com/user-attachments/assets/8161fc45-f5f9-4bdc-948d-89907d963dbc)
![스크린샷 2024-11-29 232836](https://github.com/user-attachments/assets/634e23d6-56f6-473d-bf55-1d1f81551f20)

## 예측함수
- 그리고 각각의 직선의 방정식은 w를 파라미터로 하여 나타낼 수 있다.
- 피처가 많으면 x의 개수도 늘어나지만 간단히 알아보기 위하여 피처를 하나로만 설정했다.
- 결국, 우리는 입력 데이터를 통해 최적의 w1, w0값을 구하는 것이 목표이다.
![스크린샷 2024-11-29 233013](https://github.com/user-attachments/assets/f5786ea1-30e6-4e5e-beee-d122aee2fa45)

## 결정계수(Coefficient of Determination)
- 선형 회귀분석에서 모형이 데이터의 패턴을 얼마나 효과적으로 보여주는지 수치화한 값
- 밑 식에서 Q를 SST(편차제곱합), Qe를 SSR(잔차제곱합)으로 나타내기도 한다.
- SST : 평균과 실제 데이터의 차이를 제곱하여 다 더한 것
- SSR : 예측값과 실제 데이터의 차이를 제곱하여 다 더한 것
- 따라서 결정계수는 회귀모형이 설명하는 비율을 뜻하며 높을수록 회귀모형의 성능이 좋은것이다.
![스크린샷 2024-11-29 233327](https://github.com/user-attachments/assets/22bb13fd-eead-405c-9701-11720d78238f)

## 오차함수(Error functions)
- 실제 데이터와 예측 데이터 사이에는 오차가 존재할 수 밖에 없다.
- 오차에 대한 함수를 오차함수라고 한다.

### PE(Prediction Error)
- 단순히 예측값과 실제값의 차이이다.
- 오차의 양수, 음수값이 같으면 0이 된다는 단점이 있다.
![image](https://github.com/user-attachments/assets/1ef0db8b-bff8-472b-9877-79c0b683da92)

### MAE(Mean Absolute Error)
- 예측값과 실제값의 차이에 절댓값을 씌운 후 다 더하고 평균을 낸 것이다.
![image](https://github.com/user-attachments/assets/6183569b-178e-420a-a9c1-5883e7b4faee)

### SSE(Sum of Squared Error)
- 예측값과 실제값의 차이를 제곱한 후 다 더한 것이다.
![image](https://github.com/user-attachments/assets/99af203f-e752-4473-ba8a-4e7034f0bc28)

### MSE(Mean Squared Error)
- 예측값과 실제값의 차이를 제곱한 후 다 더하고 평균을 낸 것이다.
![image](https://github.com/user-attachments/assets/2f64b042-d5fb-42b7-8930-c6a5a7ac0407)

## 손실함수(Cost functions)
- 위 오차함수를 사용해서 손실을 나타낼 수 있는 함수를 정의한다.
- 최적의 파라미터를 구해서 손실함수를 0으로 만드는 것이 목표이다.
- 앞에 1/m대신 1/2m이 들어간 이유는 미분을 했을 때 1/m만 남기기 위함이다.
![image](https://github.com/user-attachments/assets/6787e87f-b371-444a-ab7b-2b8c449911ae)
![image](https://github.com/user-attachments/assets/05a5b1ca-09f6-4ae2-a9ac-ecd231eca0b0)

## 비용함수 최소화 방법
- 대표적으로는 최소제곱법과 경사하강법이 있다.
- 최소제곱법은 비용함수를 파라미터에 대해 편미분하고 그 결과를 0으로 두고 푸는 것이다.
- 경사하강법은 편미분을 계속 시행하면서 비용을 점점 줄여나가는 것이다.
- 최소제곱법은 식이 복잡하므로 예제만 보고 경사하강법을 알아본다.


```python
import numpy as np
import matplotlib.pyplot as plt

# column vector x,y [input, output]
x = np.array([0.1, 0.4, 0.7, 1.2, 1.5, 1.7, 2.5, 2.8, 3.0, 3.8, 4.3, 4.4, 4.9]).reshape(-1, 1)
y = np.array([0.5, 0.9, 1.1, 1.5, 1.5, 2.0, 2.2, 2.8, 2.7, 3.0, 3.5, 3.7, 3.9]).reshape(-1, 1)

A = np.hstack([x**0, x])
A = np.asmatrix(A)

# Least square method
w = (A.T*A).I*A.T*y

plt.figure(figsize = (6, 4))
plt.title('Linear Regression', fontsize = 16)
plt.xlabel('X', fontsize = 12)
plt.ylabel('Y', fontsize = 12)
plt.plot(x, y, 'ko', label = "Training data")
xp = np.arange(0, 5, 0.01).reshape(-1, 1)
yp = w[0,0] + w[1,0]*xp

plt.plot(xp, yp, 'r', linewidth = 2, label = "Linear model")
plt.legend()
plt.axis('equal')
plt.grid(alpha = 0.3)
plt.xlim([0, 5])
plt.ylim([0, 4.5])
plt.show()
print(f"w값 : {w}")
```


    
![png](output_11_0.png)
    


    w값 : [[0.60879705]
     [0.68324723]]
    

## 경사하강법(Gradient Descent)
- 비용함수를 편미분한 값에 학습률을 곱하고 이를 현재의 파라미터에서 빼주는 시행을 반복함으로써 비용함수가 0으로 가게한다.
- 초기 파라미터가 오른쪽, 왼쪽 어디있든지 비용함수가 0으로 수렴하게 된다.
![image](https://github.com/user-attachments/assets/53b9cd09-e1ef-4b23-9dbf-e62697bf8bdc)
![image](https://github.com/user-attachments/assets/12e21ce4-c794-4db5-95a2-972beba4a1df)
![image](https://github.com/user-attachments/assets/1183d744-5008-463a-aad5-935c26685212)

### 학습률(Learning Rate)
- 인간이 정해주는 하이퍼파라미터로 학습속도를 결정한다.
- 학습률이 너무 크면 큰 보폭으로 이동하므로 비용함수가 발산할 위험이 있다.
- 반대로 학습률이 너무 작으면 작은 보폭으로 이동하므로 학습속도가 매우 느려진다.
- 그래서 적절한 학습률을 구해줘야 한다.
![image](https://github.com/user-attachments/assets/65085b41-2e0b-4548-8a49-f640374f2fd3)

### 학습률이 너무 작을 때
- 반복횟수를 100으로 설정했을 때 100번 반복하기까지 학습이 완료되지 않는다.


```python
import numpy as np
import matplotlib.pyplot as plt

# column vector x,y [input, output]
x = np.array([0.1, 0.4, 0.7, 1.2, 1.5, 1.7, 2.5, 2.8, 3.0, 3.8, 4.3, 4.4, 4.9]).reshape(-1, 1)
y = np.array([0.5, 0.9, 1.1, 1.5, 1.5, 2.0, 2.2, 2.8, 2.7, 3.0, 3.5, 3.7, 3.9]).reshape(-1, 1)

w = np.random.randn(2,1)    # initial parameter setting
w = np.asmatrix(w)

A = np.hstack([x**0, x])
A = np.asmatrix(A)

alpha = 0.00005              # Learning rate

for _ in range(100):         # number of epoch
    df = 2*(A.T*A*w - A.T*y)   # Partial derivative of parameter
    w = w - alpha*df           # Gradient Descent

print (w)
plt.figure(figsize = (6, 4))
plt.title('Linear Regression', fontsize = 16)
plt.xlabel('X', fontsize = 12)
plt.ylabel('Y', fontsize = 12)
plt.plot(x, y, 'ko', label = "data")

xp = np.arange(0, 5, 0.01).reshape(-1, 1)
yp = w[0,0] + w[1,0]*xp

plt.plot(xp, yp, 'r', linewidth = 2, label = "Linear model")
plt.legend()
plt.axis('equal')
plt.grid(alpha = 0.3)
plt.xlim([0, 5])
plt.show()
```

    [[-0.00222613]
     [ 0.53121517]]
    


    
![png](output_15_1.png)
    


### 학습률이 너무 클 때
- 경사를 타고 기울기가 0인 지점으로 가야되는데 보폭이 커서 위로 발산해버린다.


```python
import numpy as np
import matplotlib.pyplot as plt

# column vector x,y [input, output]
x = np.array([0.1, 0.4, 0.7, 1.2, 1.5, 1.7, 2.5, 2.8, 3.0, 3.8, 4.3, 4.4, 4.9]).reshape(-1, 1)
y = np.array([0.5, 0.9, 1.1, 1.5, 1.5, 2.0, 2.2, 2.8, 2.7, 3.0, 3.5, 3.7, 3.9]).reshape(-1, 1)

w = np.random.randn(2,1)    # initial parameter setting
w = np.asmatrix(w)

A = np.hstack([x**0, x])
A = np.asmatrix(A)

alpha = 0.05              # Learning rate

for _ in range(100):         # number of epoch
    df = 2*(A.T*A*w - A.T*y)   # Partial derivative of parameter
    w = w - alpha*df           # Gradient Descent

print (w)
plt.figure(figsize = (6, 4))
plt.title('Linear Regression', fontsize = 16)
plt.xlabel('X', fontsize = 12)
plt.ylabel('Y', fontsize = 12)
plt.plot(x, y, 'ko', label = "data")

xp = np.arange(0, 5, 0.01).reshape(-1, 1)
yp = w[0,0] + w[1,0]*xp

plt.plot(xp, yp, 'r', linewidth = 2, label = "Linear model")
plt.legend()
plt.axis('equal')
plt.grid(alpha = 0.3)
plt.xlim([0, 5])
plt.show()
```

    [[2.47148354e+101]
     [8.14323035e+101]]
    


    
![png](output_17_1.png)
    


### 학습률이 적당할 때
- 학습률이 적당하면 잘 예측하는 것을 볼 수 있다.


```python
import numpy as np
import matplotlib.pyplot as plt

# column vector x,y [input, output]
x = np.array([0.1, 0.4, 0.7, 1.2, 1.5, 1.7, 2.5, 2.8, 3.0, 3.8, 4.3, 4.4, 4.9]).reshape(-1, 1)
y = np.array([0.5, 0.9, 1.1, 1.5, 1.5, 2.0, 2.2, 2.8, 2.7, 3.0, 3.5, 3.7, 3.9]).reshape(-1, 1)

w = np.random.randn(2,1)    # initial parameter setting
w = np.asmatrix(w)

A = np.hstack([x**0, x])
A = np.asmatrix(A)

alpha = 0.005              # Learning rate

for _ in range(100):         # number of epoch
    df = 2*(A.T*A*w - A.T*y)   # Partial derivative of parameter
    w = w - alpha*df           # Gradient Descent

print (w)
plt.figure(figsize = (6, 4))
plt.title('Linear Regression', fontsize = 16)
plt.xlabel('X', fontsize = 12)
plt.ylabel('Y', fontsize = 12)
plt.plot(x, y, 'ko', label = "data")

xp = np.arange(0, 5, 0.01).reshape(-1, 1)
yp = w[0,0] + w[1,0]*xp

plt.plot(xp, yp, 'r', linewidth = 2, label = "Linear model")
plt.legend()
plt.axis('equal')
plt.grid(alpha = 0.3)
plt.xlim([0, 5])
plt.show()
```

    [[0.58866268]
     [0.68935805]]
    


    
![png](output_19_1.png)
    

